{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "import robosims\n",
    "import cv2\n",
    "import json\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RecogNet(object):\n",
    "    def __init__(self, model_name='VGG'):\n",
    "        # import pretrained model and remove the soft-max layer\n",
    "        if model_name == 'ResNet':\n",
    "            self.model = models.resnet50(pretrained=True)\n",
    "            new_classifier = nn.Sequential(*list(self.model.children())[:-1])\n",
    "            self.model = new_classifier\n",
    "        else:\n",
    "            self.model = models.vgg19(pretrained=True)\n",
    "            new_classifier = nn.Sequential(*list(self.model.classifier.children())[:-1])\n",
    "            self.model.classifier = new_classifier\n",
    "\n",
    "    def feat_extract(self, frame):\n",
    "        # normalize the input image\n",
    "        normalize = transforms.Normalize(\n",
    "           mean=[0.485, 0.456, 0.406],\n",
    "           std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        preprocess = transforms.Compose([\n",
    "           transforms.Scale(224),\n",
    "           # transforms.CenterCrop(224),\n",
    "           transforms.ToTensor(),\n",
    "           normalize\n",
    "        ])\n",
    "        image = Image.fromarray(frame)\n",
    "        img_tensor = preprocess(image)\n",
    "        # extract features\n",
    "        img_tensor.unsqueeze_(0)\n",
    "        return self.model(Variable(img_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4096, 1024)\n",
    "        self.affine2 = nn.Linear(1024, 256)\n",
    "        self.action_head = nn.Linear(256, 8)\n",
    "        self.value_head = nn.Linear(256, 1)\n",
    "\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = F.relu(self.affine1(x))\n",
    "        x2 = F.relu(self.affine2(x1))\n",
    "        action_scores = self.action_head(x2)\n",
    "        state_values = self.value_head(x2)\n",
    "        return F.softmax(action_scores), state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"ENV_TYPE\": \"living-room\",\n",
      "  \"ENV_NAME\": \"living-room-0\",\n",
      "  \"ENV_BUILD_DARWIN\": \"../../../thor-201705011400-OSXIntel64.app/Contents/MacOS/thor-201705011400-OSXIntel64\",\n",
      "  \"ENV_BUILD_LINUX\": \"unity/builds/living-room-Linux64\",\n",
      "  \"PROCESS_NAME\": \"Robot AI Platform\",\n",
      "  \"PREFIX\": \"ROBOSIMS_\",\n",
      "  \"X_DISPLAY\": \"0.0\",\n",
      "  \"SMOOTH_ANIMATION\": false,\n",
      "  \"TASK_TYPE\": \"navigation\",\n",
      "  \"TASK_TARGET\": \"balcony\",\n",
      "  \"TRAIN_WALK_VELOCITY\": 20.0,\n",
      "  \"TRAIN_TURN_VELOCITY\": 100.0,\n",
      "  \"TRAIN_ACTION_LENGTH\": 1,\n",
      "  \"TEST_WALK_VELOCITY\": 2.0,\n",
      "  \"TEST_TURN_VELOCITY\": 10.0,\n",
      "  \"TEST_ACTION_LENGTH\": 1,\n",
      "  \"AGENT_HEIGHT\": 1.8,\n",
      "  \"AGENT_RADIUS\": 0.2,\n",
      "  \"COMPUTE_DEPTH_MAP\": false,\n",
      "  \"HUMAN_CONTROL_MODE\": false,\n",
      "  \"TRAIN_PHASE\": false,\n",
      "  \"SERVER_SIDE_SCREENSHOT\": false,\n",
      "  \"PLAYER_SCREEN_WIDTH\": 300,\n",
      "  \"PLAYER_SCREEN_HEIGHT\": 300\n",
      "}\n",
      "{'uuid': '8f557b5c-db50-483a-b514-6c87ba30be2b', 'sceneName': 'FloorPlan1', 'sceneIndex': 0, 'agentPositionIndex': 22, 'startingRotation': 90.0, 'startingHorizon': 60.0, 'targetImage': 'images/FloorPlan1/spoon.png', 'targetObjectId': 'Spoon|-01.73|+00.90|-00.95', 'targetPosition': {'x': -0.6949999928474426, 'y': 0.9800000190734863, 'z': -1.7799999713897705}, 'targetAgentHorizon': 60.0, 'targetAgentRotation': 180.0}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epsolon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-42d1b85d375a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_state_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecog_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Don't infinite loop while learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsolon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_state_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecog_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epsolon' is not defined"
     ]
    }
   ],
   "source": [
    "# choose architecture\n",
    "architecture = 'ResNet'\n",
    "num_samples = 400\n",
    "if architecture == 'ResNet':\n",
    "    num_features = 2048\n",
    "else:\n",
    "    num_features = 4096\n",
    "\n",
    "# initialize environment\n",
    "env = robosims.controller.ChallengeController(\n",
    "    unity_path='../../../thor-201705011400-OSXIntel64.app/Contents/MacOS/thor-201705011400-OSXIntel64',\n",
    "    x_display=\"0.0\" # this parameter is ignored on OSX, but you must set this to the appropriate display on Linux\n",
    ")\n",
    "env.start()\n",
    "recog_net = RecogNet(architecture)\n",
    "\n",
    "# parse input\n",
    "# parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "# parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "#                     help='discount factor (default: 0.99)')\n",
    "# parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "#                     help='random seed (default: 1)')\n",
    "# parser.add_argument('--render', action='store_true',\n",
    "#                     help='render the environment')\n",
    "# parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                     help='interval between training status logs (default: 10)')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "seed = 707\n",
    "gamma = 0.99\n",
    "log_interval = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['action', 'value'])\n",
    "\n",
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=7e-4)\n",
    "\n",
    "\n",
    "def select_action(state, eps):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs, state_value = model(Variable(state))\n",
    "    if np.random.rand(1) > eps:\n",
    "        action = probs.multinomial()\n",
    "    else:\n",
    "        action = torch.from_numpy(np.random.choice(8, 1)[0])\n",
    "    print(action)\n",
    "    model.saved_actions.append(SavedAction(action, state_value))\n",
    "    return action.data\n",
    "\n",
    "def finish_episode(gamma):\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    value_loss = 0\n",
    "    rewards = []\n",
    "    for r in model.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        rewards.insert(0, R)\n",
    "    rewards = torch.Tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    for (action, value), r in zip(saved_actions, rewards):\n",
    "        reward = r - value.data[0,0]\n",
    "        action.reinforce(reward)\n",
    "        value_loss += F.smooth_l1_loss(value, Variable(torch.Tensor([r])))\n",
    "    optimizer.zero_grad()\n",
    "    final_nodes = [value_loss] + list(map(lambda p: p.action, saved_actions))\n",
    "    gradients = [torch.ones(1)] + [None] * len(saved_actions)\n",
    "    autograd.backward(final_nodes, gradients)\n",
    "    optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "\n",
    "def get_target_feature(target_name, recog_model):\n",
    "    target_image = cv2.imread(\"../../../thor-challenge-targets/\" + target_name['targetImage'])\n",
    "    target_image = cv2.resize(target_image, (300, 300))\n",
    "    return recog_model.feat_extract(target_image).squeeze()\n",
    "\n",
    "def get_state_feature(current_event, recog_model, target_feat):\n",
    "    img = current_event.frame\n",
    "    img_feat = recog_model.feat_extract(img).squeeze()\n",
    "    return torch.cat((img_feat, target_feat), 0).data.numpy()\n",
    "\n",
    "\n",
    "action_sets = ['MoveLeft', 'MoveRight', 'MoveAhead', 'MoveBack', 'LookUp', 'LookDown', 'RotateRight', 'RotateLeft']\n",
    "running_reward = 200\n",
    "reward_threshold = 100\n",
    "max_steps = 200\n",
    "epsilon = 1\n",
    "with open(\"../../../thor-challenge-targets/targets-train.json\") as f:\n",
    "    current_targets = json.loads(f.read())\n",
    "\n",
    "    for target in current_targets:\n",
    "        print(target)\n",
    "        # initialize\n",
    "        env.initialize_target(target)\n",
    "        # convert target image\n",
    "        target_feature = get_target_feature(target, recog_net)\n",
    "        event = env.step(action=dict(action='MoveAhead'))\n",
    "\n",
    "        for i_episode in count(1):\n",
    "            epsilon -= 0.01 * i_episode\n",
    "            if epsilon < 0.1:\n",
    "                epsilon = 0.1\n",
    "#             print('============== Episode: {} ==============='.format(i_episode))\n",
    "            env.initialize_target(target)\n",
    "            state = get_state_feature(event, recog_net, target_feature)\n",
    "            for t in range(max_steps):  # Don't infinite loop while learning\n",
    "                action = select_action(state, epsilon)\n",
    "                event = env.step(action=dict(action=action_sets[int(action[0, 0])]))\n",
    "                state = get_state_feature(event, recog_net, target_feature)\n",
    "                done = env.target_found()\n",
    "                if not done:\n",
    "                    reward = -1\n",
    "                else:\n",
    "                    reward = 100\n",
    "                model.rewards.append(reward)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            running_reward = running_reward * 0.99 + (t+1) * 0.01\n",
    "            finish_episode(gamma)\n",
    "            if i_episode % log_interval == 0:\n",
    "                print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "                    i_episode, t+1, running_reward))\n",
    "            if running_reward < reward_threshold:\n",
    "                print(\"Solved! Running reward is now {} and \"\n",
    "                      \"the last episode runs to {} time steps!\".format(running_reward, t+1))\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
